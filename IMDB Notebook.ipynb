{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset.\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lookup and reverse lookup\n",
    "\n",
    "lookup = imdb.get_word_index()\n",
    "reverse_lookup = {}\n",
    "for word in lookup:\n",
    "    index = lookup[word]\n",
    "    reverse_lookup[index] = word\n",
    "review_lookup = {0: 'Negative review',\n",
    "                 1: 'Positive review'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "25000\n",
      "25000\n",
      "Positive review\n",
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n",
    "# Grab a sample training and test example.\n",
    "sample_train, sample_test = X_train[0], y_train[0]\n",
    "\n",
    "# Translate this review to english.\n",
    "print(review_lookup[sample_test])\n",
    "words = []\n",
    "for index in sample_train:\n",
    "    words.append(reverse_lookup.get(index - 3, '?'))\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-6c4e63913126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-6c4e63913126>\u001b[0m in \u001b[0;36mvectorize_input\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCORPUS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Write a utility method to one-hot encode our word tensor.\n",
    "import numpy as np\n",
    "\n",
    "SIZE = len(lookup)\n",
    "CORPUS = 10000\n",
    "        \n",
    "def vectorize_input(X):\n",
    "    tensor = np.zeros((len(X), CORPUS))\n",
    "    for index, sequence in enumerate(X):\n",
    "        tensor[index, sequence] = 1\n",
    "    return tensor\n",
    "\n",
    "X_train = vectorize_input(X_train)\n",
    "X_test = vectorize_input(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we've vectorized our operations properly. Write a quick\n",
    "# utility function to decode a one hot tensor\n",
    "def decode_onehot(tensor):\n",
    "    sequence = []\n",
    "    for index, value in enumerate(tensor):     \n",
    "        if value == 1:\n",
    "            sequence.append(reverse_lookup.get(index - 3, '?'))\n",
    "    return ' '.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 5000)              50005000  \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2500)              12502500  \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1000)              2501000   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 65,559,201\n",
      "Trainable params: 65,559,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build our model. Since our input is a 10,000 length vector of all the words, \n",
    "our input should be that. We can add several dense layers to capture higher\n",
    "level features of groups of words, with the final output being a sigmoid \n",
    "on 0/1 for positive/negative.\n",
    "\n",
    "We'll use rectilinear activation for hidden units.\n",
    "\n",
    "First layer - 10000 Nodes.\n",
    "Second layer - 5000\n",
    "Third layer - 2500\n",
    "Fourth layer - 1000\n",
    "Fifth layer - 500\n",
    "Sixth layer - 100\n",
    "Seventh layer - 1\n",
    "\"\"\"\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Declare hidden layer sizes.\n",
    "HIDDEN_LAYER_SIZES = (10000, 5000, 2500, 1000, 500, 100)\n",
    "\n",
    "# Initialize a model.\n",
    "model = Sequential()\n",
    "\n",
    "# Add the layers.\n",
    "for i in range(len(HIDDEN_LAYER_SIZES) - 1):\n",
    "    input_size = HIDDEN_LAYER_SIZES[i]\n",
    "    output_size = HIDDEN_LAYER_SIZES[i+1]\n",
    "    model.add(Dense(output_size, input_dim=input_size, activation='relu'))\n",
    "    \n",
    "# Add the output layer.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Print the summary.\n",
    "model.summary()\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 17s 849us/step - loss: 0.0170 - acc: 0.9976 - val_loss: 1.3709 - val_acc: 0.8840\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 16s 815us/step - loss: 0.0032 - acc: 0.9998 - val_loss: 1.5885 - val_acc: 0.8850\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 17s 834us/step - loss: 0.0076 - acc: 0.9991 - val_loss: 1.3321 - val_acc: 0.8812\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 17s 833us/step - loss: 0.0077 - acc: 0.9991 - val_loss: 1.2776 - val_acc: 0.8800\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 18s 902us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 1.6351 - val_acc: 0.8816\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 16s 783us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 1.7518 - val_acc: 0.8778\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 17s 831us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 1.7518 - val_acc: 0.8778\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 18s 881us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 1.7518 - val_acc: 0.8778\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 19s 940us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 1.7518 - val_acc: 0.8778\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 18s 909us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 1.7518 - val_acc: 0.8778\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "progress = model.fit(x=X_train, y=y_train, validation_split=0.2, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x296a116c940>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log some of the training history, test/validation accuracy.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the history\n",
    "history = progress.history\n",
    "\n",
    "# Unpack some values of interest.\n",
    "loss = history['val_loss']\n",
    "val_accuracy = history['val_acc']\n",
    "training_accuracy = history['acc']\n",
    "\n",
    "# Plot some of our unpacked historical values.\n",
    "plt.plot(val_accuracy)\n",
    "plt.plot(training_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
